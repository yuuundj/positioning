## 로봇 위치 추적 및 측위 시스템의 연대기적 발전 과정 분석: 이론과 하드웨어 아키텍처를 중심으로

### 서론

자율 이동 로봇의 핵심 기술인 위치 추적 및 측위(Localization and Positioning) 시스템은 로봇이 자신의 위치와 방향(Pose)을 정확하게 인식하는 능력으로, 모든 후속 작업(경로 계획, 작업 수행 등)의 성공을 좌우하는 근간이다. 이 기술은 지난 수십 년간 단순한 내부 센서 기반의 추측 항법에서 시작하여, 외부 환경 정보를 확률적으로 통합하고, 종국에는 인공지능을 통해 환경을 의미적으로 이해하는 방향으로 발전해왔다. 이 발전 과정은 단순히 알고리즘의 진화에 그치지 않고, 이를 뒷받침하는 센서 기술, 컴퓨팅 하드웨어, 그리고 전체 시스템 아키텍처의 혁신과 맞물려 진행되었다. 본 보고서는 로봇 위치 추적 기술의 발전 과정을 연대기 순으로 분석하고, 각 시대별 핵심 알고리즘과 이를 구현하는 물리적 하드웨어 및 시스템 아키텍처의 변화를 심도 있게 고찰하고자 한다.

### 본론 1: 추측 항법(Dead Reckoning)과 누적 오차의 시대 (1970s ~ 1980s)

초기 이동 로봇의 위치 추적은 외부 환경에 대한 참조 없이, 로봇 내부의 고유 감각(Proprioceptive) 센서 정보에만 의존하는 추측 항법(Dead Reckoning) 방식이 지배적이었다. 이 시기의 대표적인 예는 'Shakey the Robot'으로, 로봇 공학의 여명을 연 프로젝트였다.

**[알고리즘 분석]**
핵심 알고리즘은 오도메트리(Odometry) 계산에 기반한다. 로봇의 바퀴에 장착된 회전 엔코더(Rotary Encoder)는 단위 시간당 바퀴의 회전수를 측정하고, 이 값을 바탕으로 로봇의 선속도(linear velocity)와 각속도(angular velocity)를 추정한다. 이 속도 정보를 시간에 대해 적분함으로써, 초기 위치로부터의 상대적인 이동 거리와 방향 변화를 계산하여 현재 위치를 추정하는 방식이다. 수학적으로는 현재 시점 `t`에서의 로봇의 포즈 `(x_t, y_t, θ_t)`는 이전 시점 `t-1`의 포즈와 제어 입력(속도) `u_t`의 함수로 표현된다. 이 과정은 외부 환경과의 상호작용 없이 오직 내부 측정값에만 의존하므로, 필연적으로 시스템 모델의 불확실성과 센서 측정 노이즈로 인한 오차가 시간에 따라 무한정 누적되는 치명적인 단점을 내포한다.

**[하드웨어 및 시스템 구조]**
이 시기 로봇의 하드웨어 아키텍처는 매우 단순했다.
*   **Input (Sensors):** 주된 입력 센서는 DC 모터의 축에 물리적으로 결합된 광학식 또는 자기식 회전 엔코더였다. 일부 고급 시스템에서는 초기 관성측정장치(IMU, Inertial Measurement Unit)가 사용되기도 했으나, 당시의 IMU는 크기가 크고 드리프트(drift) 오차가 매우 심각했다.
*   **Processing Unit:** 중앙 처리 장치는 8비트 혹은 16비트 마이크로컨트롤러(MCU) 또는 초기 형태의 온보드 컴퓨터(Single-Board Computer)가 주를 이루었다. 이 프로세서는 엔코더로부터 들어오는 펄스 신호를 카운팅하고, 이를 기반으로 간단한 기구학(Kinematics) 모델을 연산하여 위치를 업데이트하는 역할을 수행했다.
*   **System Pipeline:** 전체 시스템은 [엔코더 펄스 입력] -> [MCU의 카운팅 및 속도 계산] -> [기구학 모델 기반 위치 업데이트]의 단방향 순차 구조로, 외부 환경과의 피드백 루프가 존재하지 않는 개방 루프(Open-Loop) 제어 시스템이었다. 이 구조는 계산 비용이 매우 저렴하고 구현이 간단했지만, 한번 발생한 오차를 보정할 방법이 전무하여 장시간 운용 시 위치 추정의 신뢰도가 급격히 저하되었다.

### 본론 2: 확률적 로봇 공학의 등장 (1990s ~ 2000s)

추측 항법의 누적 오차 문제를 해결하기 위해, 1990년대부터 로봇 공학 분야에 확률 이론이 본격적으로 도입되기 시작했다. 로봇의 위치를 단일 값으로 확정하는 대신, '확률 분포'로 표현하고 외부 센서 정보를 이용해 이 분포를 갱신하는 베이즈 필터(Bayes Filter) 기반의 접근법이 주류를 이루었다.

**[알고리즘 분석]**
이 시대를 대표하는 알고리즘은 칼만 필터(Kalman Filter)와 파티클 필터(Particle Filter)이다.
1.  **확장 칼만 필터 (EKF, Extended Kalman Filter):** EKF는 로봇의 위치를 평균(mean)과 공분산(covariance)으로 표현되는 가우시안 분포로 모델링한다. 알고리즘은 '예측(Prediction)'과 '갱신(Update)'의 두 단계를 반복한다. 예측 단계에서는 오도메트리 정보를 이용해 로봇의 다음 위치를 추정하고, 이 과정에서 불확실성(공분산)을 증가시킨다. 갱신 단계에서는 레이저 스캐너나 소나 같은 외부 센서로 관측한 특징점(Landmark)과 지도상의 특징점 위치를 비교하여, 예측된 위치의 오차를 보정하고 불확실성을 감소시킨다. 로봇의 이동 및 관측 모델은 비선형(non-linear)이므로, 이를 선형 근사하기 위해 자코비안 행렬을 사용하는 것이 EKF의 핵심이다. EKF는 SLAM(Simultaneous Localization and Mapping) 문제, 즉 지도 제작과 위치 추정을 동시에 수행하는 문제의 초기 해법으로 널리 사용되었다.
2.  **몬테 카를로 측위 (MCL, Monte Carlo Localization) / 파티클 필터:** 파티클 필터는 가우시안 분포라는 강한 가정을 완화한 비모수적(non-parametric) 필터이다. 로봇의 위치에 대한 확률 분포를 수백에서 수천 개의 가중치(weight)를 가진 입자(particle)들의 집합으로 근사한다. 각 파티클은 로봇의 가능한 위치 가설을 의미한다. EKF와 마찬가지로 예측-갱신 사이클을 따르지만, 갱신 단계에서 실제 센서 측정값과 일치하는 파티클에 높은 가중치를 부여하고, 가중치가 낮은 파티클은 도태시키는 '재표집(Resampling)' 과정을 통해 확률 분포를 갱신한다. 이 방식은 다중 분포(multi-modal) 표현이 가능해 '납치된 로봇 문제(kidnapped robot problem)' 해결에 강점을 보였다.

**[하드웨어 및 시스템 구조]**
확률적 측위를 위해 시스템 아키텍처는 외부 센서와의 융합을 중심으로 복잡화되었다.
*   **Input (Sensors):** 오도메트리 센서와 더불어, 주변 환경을 측정하기 위한 외부 센서(Exteroceptive Sensor)가 핵심 요소로 부상했다. 저가의 초음파 센서 링(Sonar Ring), 그리고 이 시기 혁신을 이끈 2D 레이저 스캐너(LiDAR, 예: SICK LMS 시리즈)가 대표적이다. LiDAR는 270도에 달하는 넓은 시야각 내의 장애물까지의 거리를 수 밀리미터의 정밀도로 측정하여, 신뢰도 높은 환경 정보를 제공했다.
*   **Processing Unit:** EKF와 파티클 필터의 행렬 연산 및 다수 파티클 처리는 상당한 계산량을 요구했다. 따라서 x86 아키텍처 기반의 산업용 PC(Industrial PC)나 펜티엄급 프로세서를 탑재한 싱글 보드 컴퓨터가 온보드 프로세서로 사용되었다.
*   **System Pipeline:** 시스템은 [오도메트리/IMU 기반 예측] -> [LiDAR/Sonar 센서 데이터 획득] -> [데이터 연관(Data Association) 및 갱신 연산] -> [확률 분포 업데이트]의 폐쇄 루프(Closed-Loop) 구조를 형성했다. 모든 센서 데이터는 중앙 프로세서로 집중되어 융합되었으며, 이 구조는 누적 오차를 효과적으로 억제하여 로봇의 장시간 자율 주행을 가능케 한 결정적 전환점이었다.

### 본론 3: 그래프 기반 SLAM과 대규모 환경 인식 (Late 2000s ~ 2010s)

필터 기반 SLAM은 시간이 지남에 따라 연산량이 증가하고 과거의 오차를 완전히 수정하기 어렵다는 한계를 지녔다. 이에 대한 해답으로, 로봇의 전체 궤적과 지도를 하나의 거대한 최적화 문제로 푸는 그래프 기반 SLAM(Graph-based SLAM)이 부상했다.

**[알고리즘 분석]**
그래프 기반 SLAM의 핵심 아이디어는 로봇의 전체 경로와 지도 특징점을 그래프(Graph)로 표현하는 것이다. 그래프의 각 노드(Node)는 특정 시간의 로봇 포즈 또는 랜드마크의 위치를 나타낸다. 엣지(Edge)는 두 노드 간의 공간적 제약(spatial constraint)을 의미하는데, 이는 오도메트리 측정값(연속된 포즈 간의 관계)이나 센서 관측값(포즈와 랜드마크 간의 관계)으로부터 얻어진다. 로봇이 이전에 방문했던 장소를 다시 인식하는 '루프 폐쇄(Loop Closure)'가 발생하면, 그래프에 새로운 엣지가 추가되어 강력한 제약 조건을 형성한다. 전체 SLAM 문제는 이 그래프의 모든 엣지 제약 조건을 가장 잘 만족시키는 노드들의 위치를 찾는 비선형 최소제곱(Non-linear Least Squares) 최적화 문제로 귀결된다. g2o, Ceres Solver와 같은 효율적인 최적화 라이브러리의 등장은 이 접근법의 실용성을 크게 높였다.

**[하드웨어 및 시스템 구조]**
대규모 3차원 환경 인식을 위해 센서와 처리 장치의 성능이 한 단계 도약했다.
*   **Input (Sensors):** 360도 수평 및 수직 시야각을 제공하는 3D LiDAR (예: Velodyne HDL-64E)가 등장하여 풍부한 3차원 점군(Point Cloud) 데이터를 제공했다. 동시에, 저렴한 RGB-D 카메라(예: Microsoft Kinect)의 출현은 깊이 정보와 색상 정보를 결합한 SLAM 연구를 촉진시켰다. 이러한 시각 센서의 발전은 비주얼 SLAM(Visual SLAM, V-SLAM) 시대를 여는 기반이 되었다.
*   **Processing Unit:** 그래프 최적화는 주기적으로 막대한 양의 계산을 요구하므로, 멀티코어 CPU의 성능이 중요해졌다. 인텔 i5, i7 시리즈와 같은 고성능 CPU를 탑재한 온보드 컴퓨터가 표준으로 자리 잡았다. 로봇 운영체제(ROS, Robot Operating System)가 널리 보급되면서, 센서 드라이버, 데이터 처리, SLAM 알고리즘 등 각 기능이 독립적인 노드(node)로 모듈화되어 통신하는 분산 시스템 아키텍처가 정착되었다.
*   **System Pipeline:** 데이터 수집과 최적화 과정이 분리된 파이프라인이 일반화되었다. 프론트엔드(Front-end)에서는 V-SLAM의 경우 영상 특징점 추출 및 추적, LiDAR SLAM의 경우 점군 정합(Scan Matching)을 통해 실시간으로 오도메트리를 추정하고 그래프를 생성한다. 백엔드(Back-end)에서는 루프 폐쇄를 탐지하고, 일정 주기로 또는 루프 폐쇄 시점에 축적된 그래프 전체에 대해 대규모 최적화를 수행하여 맵과 궤적의 일관성을 유지한다. 이 구조는 실시간성과 전역적 정확성을 동시에 달성하는 효과적인 아키텍처로 자리매김했다.

### 본론 4: 딥러닝과 의미론적 SLAM의 시대 (Late 2010s ~ Present)

최근 로봇 측위 기술은 딥러닝과의 융합을 통해 환경을 기하학적으로만 이해하는 것을 넘어, 환경 내 객체들의 '의미(semantic)'를 이해하는 방향으로 진화하고 있다. 이는 로봇이 인간과 유사한 수준에서 공간을 인식하고 상호작용하는 데 필수적인 기술이다.

**[알고리즘 분석]**
전통적인 V-SLAM 알고리즘(예: ORB-SLAM2)에 딥러닝이 결합되면서 새로운 가능성이 열렸다.
*   **딥러닝 기반 특징점 및 기술자(Feature & Descriptor):** 합성곱 신경망(CNN, Convolutional Neural Network)을 이용하여 조명 변화나 시점 변화에 훨씬 강인한 특징점을 학습하고, 더 풍부한 정보를 담은 기술자를 추출한다.
*   **의미론적 SLAM (Semantic SLAM):** 실시간 객체 탐지(Object Detection, 예: YOLO)나 시맨틱 분할(Semantic Segmentation, 예: SegNet) 기술을 SLAM 파이프라인에 통합한다. 이를 통해 3D 지도에 '의자', '책상', '문'과 같은 의미 정보를 부여할 수 있다. 이러한 의미 정보는 데이터 연관의 정확도를 높이고, 동적 객체를 효과적으로 제거하며, 더 나아가 인간-로봇 상호작용의 기반이 된다. 예를 들어, "부엌으로 가서 컵을 가져와"와 같은 고수준 명령을 이해하고 수행할 수 있게 된다.
*   **End-to-End 측위:** 전통적인 다단계 파이프라인 대신, 원본 센서 입력(이미지 등)으로부터 직접 로봇의 포즈를 추정하는 심층 신경망을 학습시키는 연구도 활발히 진행 중이다.

**[하드웨어 및 시스템 구조]**
딥러닝 모델의 실시간 추론을 위해 이기종 컴퓨팅(Heterogeneous Computing) 아키텍처가 핵심이 되었다.
*   **Input (Sensors):** 고해상도의 글로벌 셔터 카메라, 스테레오 카메라, 이벤트 카메라 등 시각 센서의 중요성이 더욱 커졌다. 또한, 카메라, LiDAR, IMU 데이터를 시간적으로 동기화하여 상호 보완적으로 활용하는 멀티모달 센서 퓨전(Multi-modal Sensor Fusion)이 고성능 시스템의 표준이 되었다.
*   **Processing Unit:** 딥러닝 연산을 가속하기 위한 전용 하드웨어가 필수적으로 탑재된다. NVIDIA Jetson 시리즈(TX2, Xavier, Orin)와 같이 GPU가 통합된 임베디드 AI 컴퓨팅 보드가 널리 사용된다. 이 시스템에서 CPU는 ROS 노드 관리, 그래프 최적화 등 전통적인 SLAM 연산을 담당하고, 내장된 GPU는 CNN 모델 추론과 같은 병렬 연산을 전담한다.
*   **System Pipeline:** 데이터 흐름이 더욱 복잡해진다. 카메라 이미지 데이터는 GPU로 전달되어 객체 탐지 및 특징점 추출과 같은 딥러닝 기반 전처리를 거친다. 이 결과물(의미 정보, 강인한 특징점)은 CPU에서 수행되는 기하학적 SLAM 파이프라인(예: 특징점 매칭, 번들 조정)에 입력되어 최종적인 포즈 추정과 지도 생성에 활용된다. 이처럼 CPU와 GPU가 각자의 장점에 맞는 연산을 분담하여 처리하는 파이프라인은 고도의 환경 인식과 정밀한 위치 추정을 동시에 달성하는 현대 자율 로봇 시스템의 핵심 구조라 할 수 있다.

### 결론

로봇 위치 추적 및 측위 시스템의 역사는 '불확실성과의 싸움'으로 요약될 수 있다. 초기의 추측 항법은 오차 누적이라는 명백한 한계를 가졌고, 이를 극복하기 위해 확률 이론에 기반한 필터링 기법이 도입되어 외부 환경 정보를 통해 불확실성을 억제했다. 이후 그래프 기반 SLAM은 로봇의 전체 경험을 최적화하여 전역적 일관성을 확보하는 방향으로 패러다임을 전환시켰다. 그리고 현재, 딥러닝과의 융합은 기하학적 정보를 넘어 환경의 의미를 이해함으로써, 훨씬 강인하고 지능적인 측위 시스템으로의 도약을 이끌고 있다.

이러한 알고리즘의 발전은 엔코더에서 LiDAR와 고성능 카메라로 이어지는 센서 기술의 혁신, 그리고 단순 MCU에서 멀티코어 CPU를 거쳐 CPU-GPU 이기종 컴퓨팅 플랫폼으로 진화한 하드웨어 아키텍처의 발전이 있었기에 가능했다. 이론과 물리적 구현의 상호작용을 통해 로봇은 이제 미지의 환경에서도 자신의 위치를 안정적으로 파악하고, 복잡한 임무를 수행할 수 있는 능력을 갖추게 되었다. 향후 로봇 측위 기술은 클라우드 및 엣지 컴퓨팅과의 연동, 다중 로봇 간 협력 SLAM, 그리고 변화하는 환경에 평생 적응하는 라이프로깅(Lifelong) SLAM 기술로 발전하며 자율 시스템의 지평을 더욱 확장할 것으로 기대된다.

### 참고 문헌
*   **[논문]** Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., ... & Leonard, J. J. (2016). Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. *IEEE Transactions on robotics*, 32(6), 1309-1332.
*   **[논문]** Alsadik, B., & Karam, S. (2021). The simultaneous localization and mapping (SLAM): An overview. *Surveying and geospatial engineering journal*, 2(2), 79-98.
*   **[Web]** ResearchGate. (2022). The system architecture of robot localization. - [https://www.researchgate.net/figure/The-system-architecture-of-robot-localization_fig1_365040630](https://www.researchgate.net/figure/The-system-architecture-of-robot-localization_fig1_365040630)
*   **[Web]** Yan, L., et al. (2023). Robotic computing system and embodied AI evolution. *Journal of Semiconductors*. - [https://www.jos.ac.cn/en/article/doi/10.1088/1674-4926/25020034?viewType=HTML](https://www.jos.ac.cn/en/article/doi/10.1088/1674-4926/25020034?viewType=HTML)