## Visual SLAM과 GPS의 상호 보완적 결합을 통한 위치 추정 정밀도 향상 기술 분석

### 서론

자율주행 시스템, 무인 항공기(UAV), 증강현실(AR) 등 현대 기술의 핵심 요소 중 하나는 정밀하고 강건한 위치 인식(Localization) 능력이다. 이를 위해 가장 널리 사용되는 센서인 GPS(Global Positioning System)는 전역 좌표계(Global Coordinate Frame) 기준의 절대 위치를 제공하지만, 도심 협곡(Urban Canyon)이나 실내 환경에서는 신호 수신이 불가능하고 수 미터에 달하는 오차를 내재하는 한계를 가진다. 반면, Visual SLAM(Simultaneous Localization and Mapping)은 카메라를 이용하여 주변 환경의 3차원 지도와 현재 위치를 동시에 추정하는 기술로, GPS 음영 지역에서도 독립적으로 작동하며 높은 상대적 정밀도를 자랑한다. 그러나 V-SLAM은 시간에 따라 오차가 누적되는 드리프트(Drift) 현상과 초기 스케일 모호성(Scale Ambiguity) 문제에서 자유롭지 못하다. 본 보고서는 이 두 기술의 상보적 한계를 극복하기 위한 센서 퓨전(Sensor Fusion) 기술, 특히 V-SLAM과 GPS 데이터의 결합 알고리즘을 분석하고, 이를 구현하는 물리적 하드웨어 및 시스템 아키텍처를 심층적으로 조명하고자 한다.

### [알고리즘 분석]

V-SLAM, GPS, 그리고 관성 측정 장치(IMU)를 결합하는 알고리즘은 각 센서 데이터의 특성을 최대한 활용하여 상호 보완하는 방향으로 설계된다. 대표적인 접근 방식은 필터 기반의 방법과 최적화 기반의 방법으로 나뉜다.

#### **1. 확장 칼만 필터(EKF) 기반의 강결합(Tightly-Coupled) 방식**

확장 칼만 필터(EKF)는 비선형 시스템의 상태를 추정하기 위한 재귀적 베이즈 필터의 일종으로, V-SLAM과 GPS/IMU 퓨전에 널리 사용되어 왔다. 이 방식에서 시스템의 상태 벡터(State Vector)는 이동체의 위치(position), 속도(velocity), 자세(orientation)뿐만 아니라 IMU의 가속도계 및 자이로스코프 바이어스(bias), 카메라와 IMU 간의 상대적 위치 및 회전(Extrinsics) 등을 포함한다.

*   **예측(Prediction) 단계:** 고주파(high-frequency, 수백 Hz)로 측정되는 IMU의 각속도와 가속도 값을 적분하여 다음 시점의 상태를 예측한다. 이 단계는 짧은 시간 동안의 움직임을 매우 정밀하게 예측하지만, 시간이 지남에 따라 적분 오차가 빠르게 누적되는 문제를 안고 있다.
*   **갱신(Update) 단계:** 저주파(low-frequency)로 수집되는 카메라 이미지와 GPS 데이터를 사용하여 예측된 상태를 보정한다. 카메라로부터 추출된 특징점(feature points)의 재투영 오차(reprojection error)와 GPS 수신 좌표를 측정 모델(measurement model)로 사용하여 칼만 이득(Kalman Gain)을 계산하고, 이를 통해 상태 벡터의 불확실성을 감소시킨다. Alatise & Hancke (2017)의 연구에서처럼, 이 방식은 연산 부담이 상대적으로 적어 실시간 처리에 유리하지만, 비선형성이 강한 시스템에서는 선형화 오차로 인해 성능이 저하될 수 있다.

#### **2. 그래프 최적화(Graph Optimization) 기반의 전역 최적화**

최신 V-SLAM 시스템은 EKF보다 높은 정확도를 제공하는 그래프 최적화 방식을 채택하는 추세이다. 이 접근법은 일정 시간 동안의 모든 센서 측정값과 이동체의 상태를 하나의 거대한 그래프로 모델링하고, 전역적인 관점에서 오차를 최소화하는 해를 찾는다.

*   **그래프 구성:**
    *   **노드(Nodes/Vertices):** 특정 시간(Keyframe)에서의 이동체 포즈(위치와 자세)를 나타낸다.
    *   **엣지(Edges/Factors):** 노드 간의 관계, 즉 센서 측정값을 의미하는 제약(constraint)을 나타낸다. 엣지는 크게 네 종류로 구성된다.
        1.  **Visual Odometry Factors:** 연속된 키프레임 간의 상대적 움직임을 나타내는 엣지로, V-SLAM의 프론트엔드에서 계산된다.
        2.  **IMU Pre-integration Factors:** 키프레임 사이의 IMU 측정값을 사전 적분(pre-integration)하여 생성된 엣지로, 이동체의 동역학 정보를 제공한다.
        3.  **GPS Factors:** 특정 노드(포즈)에 대한 GPS 측정값을 나타내는 단항 엣지(unary edge)이다. 이 엣지는 V-SLAM으로 구축된 로컬 맵을 지구 좌표계에 정렬시키고, 누적된 드리프트를 효과적으로 보정하는 역할을 한다. Shepard & Humphreys (2014)의 연구에서는 반송파 위상차 기반 GPS(CDGPS)를 사용하여 센티미터 수준의 정밀도로 이 제약을 가하기도 했다.
        4.  **Loop Closure Factors:** 과거에 방문했던 장소를 재인식했을 때 생성되는 엣지로, 전체 궤적의 일관성을 유지하고 드리프트를 극적으로 줄인다.

*   **최적화:** 그래프가 완성되면, 모든 엣지가 나타내는 제약 조건들의 오차 합(sum of squared errors)을 최소화하는 노드들의 상태를 찾기 위해 비선형 최소제곱법(Non-linear Least Squares), 예컨대 Levenberg-Marquardt 알고리즘을 사용한다. "Gomsf" (Mascaro et al., 2018) 논문에서 제안된 방식처럼, 이 접근법은 모든 과거 정보를 활용하여 전역적으로 일관된 해를 찾기 때문에 EKF보다 훨씬 정밀하지만, 연산 복잡도가 높다는 특징이 있다.

### [하드웨어 및 시스템 구조]

이론적 알고리즘을 물리적으로 구현하기 위해서는 센서 모듈, 데이터 처리 유닛, 그리고 이들을 연결하는 시스템 아키텍처가 정교하게 설계되어야 한다.

#### **1. 센서 모듈 구성 및 물리적 배치**

*   **비전 센서(Vision Sensor):** 주로 1개(Monocular), 2개(Stereo), 또는 깊이 정보까지 측정하는 RGB-D 카메라가 사용된다. 특히 **ZED 2i**와 같은 스테레오 카메라는 양안 시차를 통해 프레임 단위로 즉시 스케일(scale)을 확정할 수 있어 V-SLAM의 초기 스케일 모호성 문제를 해결하는 데 효과적이다. 보통 초당 30~60 프레임(fps)의 이미지를 WVGA(800x480) 이상의 해상도로 수집하며, IMU와의 동기화가 매우 중요하므로 하드웨어 레벨의 트리거링 및 타임스탬프 기능이 필수적이다.
*   **관성 측정 장치(IMU):** 3축 가속도계와 3축 자이로스코프로 구성된 6축 MEMS 기반 IMU(예: **Bosch BMI088**)가 카메라와 동일한 기판에 통합된 형태로 주로 사용된다. IMU 데이터의 품질은 드리프트 누적 속도에 직접적인 영향을 미치므로, 노이즈가 적고 바이어스 안정성이 높은 고품질 센서의 선택이 중요하다.
*   **GNSS 수신기(GNSS Receiver):** 일반적인 GPS 수신기는 수 미터의 오차를 갖지만, 최근에는 **u-blox ZED-F9P** 모듈과 같이 RTK(Real-Time Kinematic) 보정 신호를 수신하여 오차를 센티미터 수준으로 줄일 수 있는 고정밀 수신기가 널리 사용된다. GPS 안테나는 위성 신호를 최대한 잘 수신할 수 있도록 이동체의 최상단에, 다른 전자 장비의 간섭이 없는 위치에 장착되어야 한다.

#### **2. 데이터 처리 파이프라인 및 연산 유닛**

센서로부터 수집된 이종(heterogeneous) 데이터는 다음과 같은 파이프라인을 통해 처리된다.

1.  **데이터 동기화:** 모든 센서(카메라, IMU, GPS) 데이터는 정확한 타임스탬프를 기준으로 정렬된다. 이는 시스템 전체 성능에 결정적인 영향을 미치는 단계로, 하드웨어 트리거나 정밀 시간 프로토콜(PTP)이 사용될 수 있다.
2.  **프론트엔드 (Tracking):** 이 단계는 주로 CPU에서 실시간으로 처리된다. V-SLAM의 경우, 이미지에서 ORB와 같은 특징점을 추출하고 이를 프레임 간에 추적하여 이동체의 상대적 움직임을 계산한다. 동시에 IMU 데이터를 사전 적분하여 동역학적 제약을 생성한다. 이 과정은 Visual-Inertial Odometry(VIO)라고도 불린다.
3.  **백엔드 (Optimization & Mapping):** 프론트엔드에서 생성된 키프레임과 제약 조건들이 누적되면, 백엔드에서 비주기적으로 그래프 최적화를 수행한다. 이 과정은 수많은 변수를 동시에 최적화해야 하므로 연산량이 막대하다. 따라서 이 작업은 **NVIDIA Jetson AGX Orin**과 같은 엣지 컴퓨팅 디바이스의 내장 GPU나, 데이터센터의 **NVIDIA A100**과 같은 고성능 GPU에 오프로드(offload)하여 병렬 처리하는 것이 일반적이다.
4.  **GPS 퓨전:** 백엔드 최적화 과정에서 수 Hz 주기로 수신되는 GPS 데이터가 그래프의 단항 엣지로 추가되어, VIO만으로는 해결할 수 없는 드리프트와 스케일 오차를 전역적으로 보정한다.

#### **3. 시스템 아키텍처**

전체 시스템은 다음과 같은 계층적 구조로 개념화할 수 있다.

*   **입력 계층 (Input Layer):** 스테레오 카메라, IMU, RTK-GNSS 수신기 모듈로 구성된다. 각 센서는 독립적으로 데이터를 생성하지만, 하드웨어 동기화 라인을 통해 동일한 시간 기준을 공유한다.
*   **처리 계층 (Processing Layer):** 임베디드 SoC(System-on-Chip)가 중심 역할을 한다.
    *   **CPU (e.g., ARM Cortex-A 시리즈):** Robot Operating System(ROS)과 같은 미들웨어를 실행하며, 각 센서 드라이버를 관리하고 데이터 동기화, V-SLAM 프론트엔드 연산, 그리고 시스템의 전반적인 로직을 제어한다.
    *   **GPU (e.g., NVIDIA Ampere 아키텍처):** CPU로부터 전달받은 데이터를 바탕으로 대규모 병렬 연산이 필요한 백엔드 최적화(Bundle Adjustment, Pose Graph Optimization)를 가속한다.
    *   **MCU/FPGA:** 경우에 따라, 나노초(nanosecond) 수준의 정밀한 센서 트리거링과 타임스탬핑을 위해 별도의 마이크로컨트롤러나 FPGA가 사용되기도 한다.
*   **출력 계층 (Output Layer):** 최종적으로 시스템은 전역 좌표계(예: WGS84)에 정렬된 6-DOF(Degrees of Freedom) 포즈(x, y, z, roll, pitch, yaw)와 3차원 희소/밀집 지도(sparse/dense map)를 실시간으로 출력한다.

### 결론

Visual SLAM과 GPS의 융합은 단순한 데이터의 평균이나 가중 합을 넘어, 각 센서의 물리적 특성과 통계적 오차 모델에 기반한 정교한 알고리즘적 결합이다. EKF와 같은 필터 기반 접근법은 실시간성과 낮은 연산 복잡도의 이점을 제공하며, 그래프 최적화 기반 접근법은 더 높은 연산 비용을 감수하는 대신 월등한 전역적 정확도와 일관성을 보장한다.

이러한 알고리즘의 성공적인 구현은 고품질의 센서를 정밀하게 동기화하고, CPU와 GPU의 역할을 명확히 분담하여 데이터 처리 파이프라인을 효율적으로 설계하는 하드웨어 및 시스템 아키텍처에 크게 의존한다. 센서의 물리적 배치, 데이터의 시간 동기화, 그리고 연산 유닛의 효율적 활용은 이론적 알고리즘의 성능을 실제 환경에서 최대한으로 이끌어내는 핵심 요소이다. 향후 이 기술은 더욱 정교한 딥러닝 기반의 특징 추출 및 데이터 융합 방식과 결합되어, 어떠한 환경에서도 끊김 없이 정밀한 위치 정보를 제공하는 방향으로 발전할 것으로 전망된다.

### 참고 문헌

*   [논문] Gomsf: Graph-optimization based multi-sensor fusion for robust uav pose estimation (R Mascaro, L Teixeira, T Hinzmann et al.) - https://ieeexplore.ieee.org/abstract/document/8460193/
*   [논문] High-precision globally-referenced position and attitude via a fusion of visual SLAM, carrier-phase-based GPS, and inertial measurements (DP Shepard, TE Humphreys) - https://ieeexplore.ieee.org/abstract/document/6851506/
*   [논문] Pose estimation of a mobile robot based on fusion of IMU data and vision data using an extended Kalman filter (MB Alatise, GP Hancke) - https://www.mdpi.com/1424-8220/17/10/2164
*   [Web] Diagram of the architecture of visual SLAM systems (ResearchGate) - https://www.researchgate.net/figure/Diagram-of-the-architecture-of-visual-SLAM-systems_fig2_360976186