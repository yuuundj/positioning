## Neural Map (NeRF) 기반 실시간 다중 객체 매핑 기술 분석: RO-MAP 심층 분석 보고서

### **서론**

최근 로보틱스 및 자율주행 기술의 발전에 따라, 주변 환경을 3차원으로 인식하고 실시간으로 지도를 구축하는 기술의 중요성이 증대되고 있다. 전통적인 SLAM(Simultaneous Localization and Mapping) 기술은 주로 점 구름(Point Cloud)이나 복셀(Voxel) 그리드 형태로 환경을 표현했으나, 이는 기하학적 정보에 치중하여 객체의 사실적인 텍스처나 광원 효과를 표현하는 데 한계가 있었다. 이러한 문제를 해결하기 위해, 2D 이미지로부터 3D 장면을 사실적으로 렌더링하는 Neural Radiance Fields (NeRF) 기술이 주목받고 있다. 본 보고서는 NeRF를 활용하여 실시간으로 다중 객체 지도를 생성하는 'RO-MAP' 알고리즘을 중심으로, 그 핵심 이론과 이를 구현하는 물리적 하드웨어 및 시스템 아키텍처를 심층적으로 분석하고자 한다. 이를 통해 이론적 모델이 실제 로보틱스 시스템에 어떻게 통합되고 작동하는지에 대한 통찰을 제공하는 것을 목표로 한다.

### **[1] 알고리즘 분석: RO-MAP의 수학적 원리**

RO-MAP (Real-Time Multi-Object Mapping with Neural Radiance Fields)은 단안 카메라(Monocular Camera) 입력만을 사용하여 동적인 환경 내 다수의 객체를 실시간으로 3D 매핑하는 것을 목표로 개발된 알고리즘이다. 해당 기술의 핵심은 두 가지 주요 프레임워크의 유기적인 결합에 있다: **객체 SLAM(Object SLAM)**과 **다중 객체 NeRF 시스템**.

1.  **객체 SLAM: 위치 추정 및 객체 탐지**
    RO-MAP의 기반이 되는 SLAM 시스템은 ORB-SLAM2를 변형하여 사용한다. 이 모듈은 단안 카메라로부터 입력되는 연속적인 이미지 프레임을 분석하여 두 가지 핵심 정보를 추출한다. 첫째, 카메라 자체의 6-DoF(Degrees of Freedom) 포즈(위치 및 방향)를 실시간으로 추정한다. 둘째, 2D 이미지 내에서 의미론적 분할(Semantic Segmentation) 및 객체 탐지 알고리즘을 통해 식별된 객체들의 3차원 경계 상자(Bounding Box)와 포즈를 추정한다. 이 과정은 특징점(Feature Point) 기반의 시각적 주행 거리 측정(Visual Odometry)과 최적화 기법인 번들 조정(Bundle Adjustment)을 통해 이루어지며, 이를 통해 카메라와 객체의 위치 관계를 지속적으로 보정한다. 즉, 이 단계의 최종 산출물은 각 타임스탬프(Timestamp)에서의 카메라 포즈와 각 객체의 고유 ID, 3D 위치, 크기 정보다.

2.  **다중 객체 NeRF: 사실적 3D 복원**
    객체 SLAM을 통해 각 객체의 위치와 경계가 정의되면, RO-MAP은 각 객체별로 독립적인 NeRF 모델을 할당하여 학습을 진행한다. NeRF의 핵심 원리는 3차원 공간상의 한 점 (x, y, z)과 바라보는 방향 (θ, φ)을 입력받아 해당 지점의 색상(RGB)과 밀도(σ, Volume Density) 값을 출력하는 MLP(Multi-Layer Perceptron) 신경망을 학습하는 것이다. 수학적으로 표현하면, NeRF는 함수 FΘ: (x, d) → (c, σ)를 근사하는 과정이다. 여기서 x는 3D 위치, d는 뷰 방향, c는 색상, σ는 밀도이며, Θ는 신경망의 가중치(weight)를 의미한다.

    RO-MAP은 각 객체에 대해 별도의 소규모 NeRF 모델을 병렬적으로 학습시킨다. 특정 객체의 3D 모델을 렌더링하기 위해, 해당 객체의 경계 상자를 통과하는 광선(Ray)을 따라 샘플링된 점들의 위치와 방향 정보를 해당 객체 전용 NeRF 모델에 입력한다. 각 점에서 출력된 색상과 밀도 값은 볼륨 렌더링(Volume Rendering) 기법을 통해 최종 픽셀 색상으로 통합된다. 볼륨 렌더링 수식은 다음과 같다.
    C(r) = ∫[t_n to t_f] T(t) * σ(r(t)) * c(r(t), d) dt
    여기서 T(t) = exp(-∫[t_n to t] σ(r(s)) ds) 이다.

    이러한 병렬적 접근 방식은 전체 장면에 대해 단일 거대 NeRF 모델을 사용하는 것에 비해 계산 효율성을 크게 향상시킨다. 특히, NVIDIA가 개발한 `tiny-cuda-nn` 라이브러리를 기반으로 구현되어 소규모 MLP를 CUDA 코어에서 매우 빠르게 학습하고 추론할 수 있게 함으로써 실시간성을 확보했다.

### **[2] 하드웨어 및 시스템 아키텍처**

RO-MAP의 실시간 성능은 소프트웨어 알고리즘의 효율성뿐만 아니라, 이를 뒷받침하는 하드웨어 구성 및 시스템 아키텍처에 크게 의존한다. 전체 시스템은 '센서 입력 → CPU 기반 SLAM 처리 → GPU 기반 NeRF 렌더링'의 명확한 파이프라인으로 구성된다.

1.  **입력 센서 및 데이터 플로우**
    시스템의 가장 앞단에는 단안 RGB 카메라가 위치한다. 이 카메라는 주변 환경의 연속적인 비디오 스트림을 캡처하여 시스템의 주 입력으로 제공한다. 카메라에서 출력된 이미지 프레임은 시스템의 메인 프로세서로 전송되며, 여기서 SLAM 모듈과 NeRF 모듈로 분기되어 전달된다. 별도의 LiDAR나 깊이 센서 없이 단안 카메라만 사용하므로, 하드웨어 구성이 단순하고 비용 효율적인 장점이 있다. 하지만 이는 깊이 정보의 부재로 인한 SLAM의 불안정성이나 스케일 모호성(Scale Ambiguity) 문제를 야기할 수 있으며, 알고리즘은 이를 특징점 매칭과 기하학적 제약을 통해 극복해야 한다.

2.  **중앙 처리 장치 (CPU)의 역할: SLAM 연산**
    입력된 이미지 프레임은 우선적으로 CPU에서 실행되는 객체 SLAM 모듈로 전달된다. ORB-SLAM2와 같은 특징점 기반 SLAM 알고리즘은 이미지 전처리, 특징점 추출(ORB), 매칭, 루프 폐쇄(Loop Closure) 등 순차적이고 논리적인 연산이 많아 CPU에서 처리하는 것이 효율적이다.
    *   **프로세스:** CPU는 이미지에서 수백 개의 ORB 특징점을 추출하고, 이전 프레임과의 매칭을 통해 카메라의 움직임을 계산한다. 동시에, YOLO와 같은 객체 탐지 모델(CPU 또는 저사양 GPU에서 실행 가능)을 통해 객체를 식별하고, 이 정보를 SLAM의 맵 포인트와 결합하여 객체의 3D 경계 상자를 추정한다.
    *   **하드웨어 사양:** 실시간 처리를 위해서는 다중 코어 및 높은 클럭 속도를 가진 Intel Core i7 또는 AMD Ryzen 7 이상의 고성능 CPU가 요구된다. 이는 SLAM의 추적(Tracking), 지역 매핑(Local Mapping), 루프 폐쇄(Loop Closing) 스레드를 동시에 안정적으로 실행하기 위함이다.

3.  **그래픽 처리 장치 (GPU)의 역할: NeRF 병렬 연산**
    CPU 기반 SLAM 모듈에서 계산된 카메라 포즈와 객체별 경계 상자 정보는 GPU로 전달되어 NeRF 기반 3D 복원 작업을 수행한다. NeRF는 수많은 광선(Ray)에 대해 독립적인 MLP 연산을 수행해야 하므로, 대규모 병렬 처리에 특화된 GPU가 핵심적인 역할을 담당한다.
    *   **프로세스:** RO-MAP은 `tiny-cuda-nn`을 활용하여 각 객체에 대한 NeRF 모델을 GPU 메모리에 상주시키고 병렬로 학습/추론한다. 렌더링이 필요한 시점에, SLAM으로부터 받은 포즈 정보를 바탕으로 가상 카메라의 광선을 생성하고, 각 광선에 대한 샘플링 및 MLP 추론을 수만 개의 CUDA 코어에서 동시에 실행한다. 이 결과물들을 볼륨 렌더링 공식을 통해 조합하여 최종적인 2D 이미지를 생성한다.
    *   **하드웨어 사양:** `tiny-cuda-nn`은 NVIDIA GPU의 텐서 코어(Tensor Core)를 적극적으로 활용하므로, 실시간 성능을 위해서는 NVIDIA RTX 2080 이상, 혹은 데이터센터급으로는 A100과 같은 텐서 코어가 탑재된 최신 GPU가 필수적이다. GPU의 VRAM 용량 또한 동시에 처리할 수 있는 객체의 수와 NeRF 모델의 복잡도를 결정하는 중요한 요소다. 예를 들어, 한 객체당 수십 MB의 작은 모델을 사용하더라도 10개 이상의 객체를 동시에 처리하려면 수 GB의 VRAM이 필요하다.

**시스템 아키텍처 다이어그램:**


```
[Monocular Camera] --(Image Stream)--> [CPU] --(Pose & BBox Info)--> [GPU]
       |                                  |                              |
       |                                  |                              |
       +-----> [Object SLAM Module]       +-----> [Multi-Object NeRF Module]
             (ORB-SLAM2 based)                 (tiny-cuda-nn based)
             - Camera Pose Estimation          - Parallel NeRF Training
             - Object Detection                - Rendering per Object
             - 3D Bounding Box Estimation      - Volume Integration
                                                   |
                                                   |
                                                   V
                                          [3D Object Map]
```


### **결론**

RO-MAP은 전통적인 기하학 기반 SLAM과 최신 신경망 기반 렌더링 기술인 NeRF를 성공적으로 융합하여, 실시간 다중 객체 3D 매핑의 새로운 패러다임을 제시했다. 이론적으로는 SLAM을 통해 공간의 구조적 정보를 빠르고 강건하게 파악하고, NeRF를 통해 각 객체의 사실적인 외형을 복원함으로써 기존 방식의 한계를 극복했다.

이러한 이론의 실제 구현은 CPU와 GPU의 역할을 명확히 분담하는 이기종 컴퓨팅(Heterogeneous Computing) 아키텍처를 통해 가능해졌다. 순차적이고 논리적인 연산이 많은 SLAM은 CPU에, 대규모 병렬 처리가 필수적인 NeRF는 GPU에 할당함으로써 시스템 전체의 처리량을 극대화했다. 특히 `tiny-cuda-nn`과 같은 최적화된 라이브러리의 채택은 NeRF의 고질적인 연산 비용 문제를 해결하고 실시간성을 확보하는 결정적인 요소로 작용했다.

다만, 단안 카메라에만 의존하는 현재 구조는 초기화 과정에서의 스케일 모호성이나 조명 변화 및 빠른 움직임에 대한 취약성이라는 내재적 한계를 가진다. 향후 IMU(관성 측정 장치)나 깊이 센서와의 퓨전을 통해 시스템의 강건성을 향상시키고, 더욱 복잡하고 동적인 환경에서도 안정적으로 작동하도록 발전할 가능성이 높다. RO-MAP은 이론과 실제 구현의 조화를 통해 로보틱스 분야의 환경 인식 기술을 한 단계 진보시킨 중요한 사례라 평가할 수 있다.

### **참고 문헌**

*   **[논문]** Han, X., Liu, H., Ding, Y., & Yang, L. (2023). RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields. *arXiv preprint arXiv:2304.05735*. - https://arxiv.org/abs/2304.05735
*   **[Web]** XiaoHan-Git/RO-MAP GitHub Repository. - https://github.com/XiaoHan-Git/RO-MAP